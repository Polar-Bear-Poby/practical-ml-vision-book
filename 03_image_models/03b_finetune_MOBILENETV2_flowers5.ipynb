{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lLz90Lmve0yW",
        "outputId": "0a7e8e84-38cb-4e8a-b5b6-2d0280375900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?name=5+flowers%EC%97%90+%EB%8C%80%ED%95%B4+MobileNetV2+%ED%8C%8C%EC%9D%B8+%ED%8A%9C%EB%8B%9D%28%EC%9D%B4%EB%AF%B8%EC%A7%80+%EB%B6%84%EB%A5%98%29&download_url=https%3A%2F%2Fgithub.com%2Fychoi-kr%2Fpractical-ml-vision-book-ko%2Fraw%2Fmaster%2F03_image_models%2F03b_finetune_MOBILENETV2_flowers5.ipynb\"><img src=\"https://raw.githubusercontent.com/ychoi-kr/practical-ml-vision-book-ko/master/logo-cloud.png\"/>AI Platform Notebook에서 실행</a></td><td><a target=\"_blank\" href=\"https://colab.research.google.com/github/ychoi-kr/practical-ml-vision-book-ko/blob/master/03_image_models/03b_finetune_MOBILENETV2_flowers5.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Google Colab에서 실행</a></td><td><a target=\"_blank\" href=\"https://github.com/ychoi-kr/practical-ml-vision-book-ko/blob/master/03_image_models/03b_finetune_MOBILENETV2_flowers5.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />GitHub에서 소스 보기</a></td><td><a href=\"https://raw.githubusercontent.com/ychoi-kr/practical-ml-vision-book-ko/master/03_image_models/03b_finetune_MOBILENETV2_flowers5.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />노트북 내려받기</a></td></table><br/><br/><h1>5 flowers에 대해 MobileNetV2 파인 튜닝(이미지 분류)</h1>이 노트북은 TPU 또는 GPU로 실행하도록 설정되어 있다. TPUv3 에서 실행했다. TPUv2(Colab)나 GPU 등 메모리가 적은 하드웨어에서 실행할 경우 다음 [구성] 섹션에서 배치 크기나 이미지 크기를 작게 설정할 필요가 있는 경우가 있다."
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import urllib\n",
        "from IPython.display import Markdown as md\n",
        "_nb_loc = \"03_image_models/03b_finetune_MOBILENETV2_flowers5.ipynb\"\n",
        "_nb_title = \"5 flowers에 대해 MobileNetV2 파인 튜닝(이미지 분류)\"\n",
        "_nb_message = \"이 노트북은 TPU 또는 GPU로 실행하도록 설정되어 있다. TPUv3 에서 실행했다. TPUv2(Colab)나 GPU 등 메모리가 적은 하드웨어에서 실행할 경우 다음 [구성] 섹션에서 배치 크기나 이미지 크기를 작게 설정할 필요가 있는 경우가 있다.\"\n",
        "### no need to change any of this\n",
        "_icons=[\"https://raw.githubusercontent.com/ychoi-kr/practical-ml-vision-book-ko/master/logo-cloud.png\", \"https://www.tensorflow.org/images/colab_logo_32px.png\", \"https://www.tensorflow.org/images/GitHub-Mark-32px.png\", \"https://www.tensorflow.org/images/download_logo_32px.png\"]\n",
        "_links=[\"https://console.cloud.google.com/ai-platform/notebooks/deploy-notebook?\" + urllib.parse.urlencode({\"name\": _nb_title, \"download_url\": \"https://github.com/ychoi-kr/practical-ml-vision-book-ko/raw/master/\"+_nb_loc}), \"https://colab.research.google.com/github/ychoi-kr/practical-ml-vision-book-ko/blob/master/{0}\".format(_nb_loc), \"https://github.com/ychoi-kr/practical-ml-vision-book-ko/blob/master/{0}\".format(_nb_loc), \"https://raw.githubusercontent.com/ychoi-kr/practical-ml-vision-book-ko/master/{0}\".format(_nb_loc)]\n",
        "md(\"\"\"<table class=\"tfo-notebook-buttons\" align=\"left\"><td><a target=\"_blank\" href=\"{0}\"><img src=\"{4}\"/>AI Platform Notebook에서 실행</a></td><td><a target=\"_blank\" href=\"{1}\"><img src=\"{5}\" />Google Colab에서 실행</a></td><td><a target=\"_blank\" href=\"{2}\"><img src=\"{6}\" />GitHub에서 소스 보기</a></td><td><a href=\"{3}\"><img src=\"{7}\" />노트북 내려받기</a></td></table><br/><br/><h1>{8}</h1>{9}\"\"\".format(_links[0], _links[1], _links[2], _links[3], _icons[0], _icons[1], _icons[2], _icons[3], _nb_title, _nb_message))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "83HQKgMce0yZ",
        "outputId": "8103f880-41a3-4548-823f-8aff569ddb91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script wheel is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.7 are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script markdown_py is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script tensorboard is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "explainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\n",
            "tfx-bsl 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 1.0.0 which is incompatible.\n",
            "tfx-bsl 1.4.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.31.0 which is incompatible.\n",
            "tfx-bsl 1.4.0 requires numpy<1.20,>=1.16, but you have numpy 1.21.4 which is incompatible.\n",
            "tfx-bsl 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
            "tfx-bsl 1.4.0 requires tensorflow-metadata<1.5,>=1.4, but you have tensorflow-metadata 1.5.0 which is incompatible.\n",
            "tensorflow-transform 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 1.0.0 which is incompatible.\n",
            "tensorflow-transform 1.4.0 requires numpy<1.20,>=1.16, but you have numpy 1.21.4 which is incompatible.\n",
            "tensorflow-transform 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.1 which is incompatible.\n",
            "tensorflow-transform 1.4.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,<2.7,>=1.15.2, but you have tensorflow 2.7.0 which is incompatible.\n",
            "tensorflow-transform 1.4.0 requires tensorflow-metadata<1.5.0,>=1.4.0, but you have tensorflow-metadata 1.5.0 which is incompatible.\n",
            "tensorflow-metadata 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 1.0.0 which is incompatible.\n",
            "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.7.0 which is incompatible.\n",
            "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.22.0 which is incompatible.\n",
            "pandas-profiling 3.0.0 requires tangled-up-in-unicode==0.1.0, but you have tangled-up-in-unicode 0.2.0 which is incompatible.\n",
            "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.4 which is incompatible.\n",
            "google-cloud-recommendations-ai 0.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.2.2 which is incompatible.\n",
            "apache-beam 2.34.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\n",
            "apache-beam 2.34.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.2 which is incompatible.\n",
            "apache-beam 2.34.0 requires numpy<1.21.0,>=1.14.3, but you have numpy 1.21.4 which is incompatible.\n",
            "apache-beam 2.34.0 requires pyarrow<6.0.0,>=0.15.1, but you have pyarrow 6.0.1 which is incompatible.\n",
            "apache-beam 2.34.0 requires typing-extensions<4,>=3.7.0, but you have typing-extensions 4.0.1 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install --user --quiet --force keras-adamw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X-jbjAIe0yZ",
        "outputId": "4047af26-3d90-4ecd-f582-21f36cbf37dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# adamw 패키지를 선택하기 위해 커널을 재시작\n",
        "import IPython\n",
        "\n",
        "IPython.Application.instance().kernel.do_shutdown(True)  # 자동으로 커널을 재시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "7lFb0xdNe0yZ",
        "outputId": "1ae34751-3b1e-41c3-8cab-8ac5457055f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.7.0\n"
          ]
        }
      ],
      "source": [
        "import math, re, os, sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apRcdeR6e0ya"
      },
      "outputs": [],
      "source": [
        "# 경고! 이 호출은 *모든* TensorFlow 호출 전에 와야 한다.\n",
        "# AdamW를 찾을 수 없다는 오류가 발생하면 커널을 다시 시작하고 이전 셀에서 시작하라.\n",
        "os.environ['TF_KERAS'] = '1'  # AdamW\n",
        "from keras_adamw import AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYXzglrke0ya"
      },
      "source": [
        "# TPU 또는 GPU 감지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXQd6LDPe0yb",
        "outputId": "37621dca-30a3-4494-91b9-1e1ca06ade00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
            "REPLICAS:  1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-04 17:32:27.278316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-12-04 17:32:27.283342: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
            "2021-12-04 17:32:27.284096: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n",
            "2021-12-04 17:32:27.285920: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "try: # TPU를 감지\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
        "    strategy = tf.distribute.TPUStrategy(tpu)\n",
        "except ValueError:  # GPU들 또는 여러 대의 GPU 머신을 감지\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqJFT9hze0yc"
      },
      "source": [
        "# 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WoEaXAle0yc",
        "outputId": "c5ed84a0-00c7-4e03-d186-dc15af08d432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Learning rate schedule: 1e-05 to 0.0001 to 2.21e-05\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "GCS_DS_PATH = \"gs://practical-ml-vision-book/flowers_5_tfr\"\n",
        "\n",
        "# TPUv3에 대한 설정. TPUv2(Colab) 또는 GPU와 같이 메모리가 적은 하드웨어에서\n",
        "# 실행하는 경우 더 낮은 BATCH_SIZE 및 IMAGE_SIZE 값을 사용해야 할 수 있다.\n",
        "\n",
        "IMAGE_SIZE = [224, 224]  # flowers104 데이터셋에서 사용 가능한 이미지 크기: 512x512, 331x331, 224x224, 192x192\n",
        "EPOCHS = 13\n",
        "\n",
        "# 파인 튜닝을 위한 학습률 스케줄: trainable=True 사용(최고 검증 정확도 0.91)\n",
        "#BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "#LR_START = 0.00001\n",
        "#LR_MAX = 0.000025 * strategy.num_replicas_in_sync\n",
        "#LR_MIN = 0.00001\n",
        "#LR_RAMPUP_EPOCHS = 3\n",
        "#LR_SUSTAIN_EPOCHS = 1\n",
        "#LR_EXP_DECAY = .8\n",
        "\n",
        "# AdamW를 사용한 파인 튜닝을 위한 학습률 스케줄: trainable=True 사용(최고 검증 정확도 0.92)\n",
        "BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
        "LR_START = 0.00001\n",
        "LR_MAX = 0.0001 * strategy.num_replicas_in_sync\n",
        "LR_MIN = 0.00001\n",
        "LR_RAMPUP_EPOCHS = 3\n",
        "LR_SUSTAIN_EPOCHS = 0\n",
        "LR_EXP_DECAY = .8\n",
        "\n",
        "# 전이 학습을 위한 학습률 스케줄: trainable=False 사용(최고 검증 정확도 0.90)\n",
        "#BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "#LR_START = 0.00001\n",
        "#LR_MAX = 0.00075 * strategy.num_replicas_in_sync #(참고: 책의 그래프를 복제하려면 0.00007과 trainable=True로 설정)\n",
        "#LR_MIN = 0.00001\n",
        "#LR_RAMPUP_EPOCHS = 0\n",
        "#LR_SUSTAIN_EPOCHS = 0\n",
        "#LR_EXP_DECAY = .8\n",
        "\n",
        "GCS_PATH_SELECT = {  # 사용 가능한 이미지 크기\n",
        "    192: GCS_DS_PATH + '/tfrecords-jpeg-192x192',\n",
        "    224: GCS_DS_PATH + '/tfrecords-jpeg-224x224',\n",
        "    331: GCS_DS_PATH + '/tfrecords-jpeg-331x331',\n",
        "    512: GCS_DS_PATH + '/tfrecords-jpeg-512x512'\n",
        "}\n",
        "GCS_PATH = GCS_PATH_SELECT[IMAGE_SIZE[0]] + '/*.tfrec'\n",
        "filenames = tf.io.gfile.glob(GCS_PATH)\n",
        "validation_split = 0.19\n",
        "split = len(filenames) - int(len(filenames) * validation_split)\n",
        "TRAINING_FILENAMES = filenames[:split]\n",
        "VALIDATION_FILENAMES = filenames[split:]\n",
        "\n",
        "CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
        "\n",
        "def lrfn(epoch):\n",
        "    if epoch < LR_RAMPUP_EPOCHS:\n",
        "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
        "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
        "        lr = LR_MAX\n",
        "    else:\n",
        "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
        "    return lr\n",
        "    \n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
        "\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y = [lrfn(x) for x in rng]\n",
        "plt.plot(rng, y)\n",
        "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6czN2U_e0yd"
      },
      "source": [
        "## 시각화 유틸리티\n",
        "data -> pixels, nothing of much interest for the machine learning practitioner in this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn7uAUlse0yd"
      },
      "outputs": [],
      "source": [
        "# numpy와 matplotlib 기본값\n",
        "np.set_printoptions(threshold=15, linewidth=80)\n",
        "\n",
        "def batch_to_numpy_images_and_labels(data):\n",
        "    images, labels = data\n",
        "    numpy_images = images.numpy()\n",
        "    numpy_labels = labels.numpy()\n",
        "    if numpy_labels.dtype == object:  # 이 경우 이진 문자열. 이것들은 이미지 ID 문자열임\n",
        "        numpy_labels = [None for _ in enumerate(numpy_images)]\n",
        "    # 레이블이 없고 이미지 ID만 있는 경우, 레이블에 대해 None을 반환(테스트 데이터의 경우)\n",
        "    return numpy_images, numpy_labels\n",
        "\n",
        "def title_from_label_and_target(label, correct_label):\n",
        "    if correct_label is None:\n",
        "        return CLASSES[label], True\n",
        "    correct = (label == correct_label)\n",
        "    return \"{} [{}{}{}]\".format(CLASSES[label], 'OK' if correct else 'NO', u\"\\u2192\" if not correct else '',\n",
        "                                CLASSES[correct_label] if not correct else ''), correct\n",
        "\n",
        "def display_one_flower(image, title, subplot, red=False, titlesize=16):\n",
        "    plt.subplot(*subplot)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "    if len(title) > 0:\n",
        "        plt.title(title, fontsize=int(titlesize) if not red else int(titlesize/1.2), color='red' if red else 'black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
        "    return (subplot[0], subplot[1], subplot[2]+1)\n",
        "    \n",
        "def display_batch_of_images(databatch, predictions=None):\n",
        "    \"\"\"This will work with:\n",
        "    display_batch_of_images(images)\n",
        "    display_batch_of_images(images, predictions)\n",
        "    display_batch_of_images((images, labels))\n",
        "    display_batch_of_images((images, labels), predictions)\n",
        "    \"\"\"\n",
        "    # 데이터터\n",
        "    images, labels = batch_to_numpy_images_and_labels(databatch)\n",
        "    if labels is None:\n",
        "        labels = [None for _ in enumerate(images)]\n",
        "        \n",
        "    # 정사각형 또는 정사각형에 가까운 직사각형에 맞지 않는 데이터를 삭제\n",
        "    rows = int(math.sqrt(len(images)))\n",
        "    cols = len(images)//rows\n",
        "        \n",
        "    # 크기와 간격\n",
        "    FIGSIZE = 13.0\n",
        "    SPACING = 0.1\n",
        "    subplot=(rows,cols,1)\n",
        "    if rows < cols:\n",
        "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
        "    else:\n",
        "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
        "    \n",
        "    # 디스플레이\n",
        "    for i, (image, label) in enumerate(zip(images[:rows*cols], labels[:rows*cols])):\n",
        "        title = '' if label is None else CLASSES[label]\n",
        "        correct = True\n",
        "        if predictions is not None:\n",
        "            title, correct = title_from_label_and_target(predictions[i], label)\n",
        "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols)*40+3  # 1x1에서 10x10 사이의 이미지에 잘 맞는 마법 공식\n",
        "        subplot = display_one_flower(image, title, subplot, not correct, titlesize=dynamic_titlesize)\n",
        "    \n",
        "    # 레이아웃\n",
        "    plt.tight_layout()\n",
        "    if label is None and predictions is None:\n",
        "        plt.subplots_adjust(wspace=0, hspace=0)\n",
        "    else:\n",
        "        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
        "    plt.show()\n",
        "\n",
        "def display_confusion_matrix(cmat, score, precision, recall):\n",
        "    #plt.figure(figsize=(15,15))\n",
        "    ax = plt.gca()\n",
        "    ax.matshow(cmat, cmap='Reds')\n",
        "    ax.set_xticks(range(len(CLASSES)))\n",
        "    ax.set_xticklabels(CLASSES)\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
        "    ax.set_yticks(range(len(CLASSES)))\n",
        "    ax.set_yticklabels(CLASSES)\n",
        "    plt.setp(ax.get_yticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "    #titlestring = \"\"\n",
        "    #if score is not None:\n",
        "    #    titlestring += 'f1 = {:.3f} '.format(score)\n",
        "    #if precision is not None:\n",
        "    #    titlestring += '\\nprecision = {:.3f} '.format(precision)\n",
        "    #if recall is not None:\n",
        "    #    titlestring += '\\nrecall = {:.3f} '.format(recall)\n",
        "    #if len(titlestring) > 0:\n",
        "    #    ax.text(101, 1, titlestring, fontdict={'fontsize': 18, 'horizontalalignment':'right', 'verticalalignment':'top', 'color':'#804040'})\n",
        "    plt.show()\n",
        "    \n",
        "def display_training_curves(training, validation, title, subplot, zoom_pcent=None, ylim=None):\n",
        "    # zoom_pcent: X는 데이터 포인트의 마지막 X%에 대해 y축을 자동 조정\n",
        "    if subplot%10==1:  # 첫 번째 호출에서 서브플롯 설정\n",
        "        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
        "        plt.tight_layout()\n",
        "    ax = plt.subplot(subplot)\n",
        "    ax.set_facecolor('#F8F8F8')\n",
        "    ax.plot(training)\n",
        "    ax.plot(validation, '--')\n",
        "    ax.set_title('model '+ title)\n",
        "    ax.set_ylabel(title)\n",
        "    if zoom_pcent is not None:\n",
        "        ylen = len(training)*(100-zoom_pcent)//100\n",
        "        ymin = min([min(training[ylen:]), min(validation[ylen:])])\n",
        "        ymax = max([max(training[ylen:]), max(validation[ylen:])])\n",
        "        ax.set_ylim([ymin-(ymax-ymin)/20, ymax+(ymax-ymin)/20])\n",
        "    if ylim is not None:\n",
        "        ymin = ylim[0]\n",
        "        ymax = ylim[1]\n",
        "        ax.set_ylim([ymin-(ymax-ymin)/20, ymax+(ymax-ymin)/20])\n",
        "    ax.set_xlabel('epoch')\n",
        "    ax.legend(['train', 'valid.'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8CEL6Jqe0ye"
      },
      "source": [
        "# 데이터셋"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "g3eoSTFle0ye"
      },
      "outputs": [],
      "source": [
        "def decode_image(image_data):\n",
        "    image = tf.image.decode_jpeg(image_data, channels=3)  # [0,255] 범위의 uint8 형식으로 디코딩된 이미지\n",
        "    image = tf.reshape(image, [*IMAGE_SIZE, 3])  # TPU에 필요한 명시적 크기\n",
        "    return image\n",
        "\n",
        "def read_tfrecord(example):\n",
        "    TFREC_FORMAT = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string은 바이트스트링을 의미\n",
        "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # [] 모양은 단일 요소를 의미\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n",
        "    image = decode_image(example['image'])\n",
        "    label = tf.cast(example['class'], tf.int32)\n",
        "    return image, label  # (image, label) 쌍의 데이터셋을 반환\n",
        "\n",
        "def load_dataset(filenames, ordered=False):\n",
        "    # TFRecords에서 읽는다. 최적의 성능을 위해 한 번에 여러 파일에서 읽고 데이터\n",
        "    # 순서를 무시한다. 어차피 데이터를 섞을 것이므로 순서는 중요하지 않다.\n",
        "\n",
        "    ignore_order = tf.data.Options()\n",
        "    if not ordered:\n",
        "        ignore_order.experimental_deterministic = False  # 순서를 비활성화, 속도 향상\n",
        "\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)  # 여러 파일에서 읽기를 자동으로 인터리빙\n",
        "    dataset = dataset.with_options(ignore_order)  # 원래 순서가 아닌 스트리밍되는 즉시 데이터를 사용\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
        "    # (image, label) 쌍의 데이터셋을 반환\n",
        "    return dataset\n",
        "\n",
        "def data_augment(image, label):\n",
        "    # 데이터 증강. 다음 함수(아래)의 dataset.prefetch(AUTO) 문 덕분에 이는\n",
        "    # 기본적으로 TPU에서 무료로 이뤄진다. 데이터 파이프라인 코드는 TPU 자체가\n",
        "    # 기울기를 계산하는 동안 TPU의 \"CPU\" 부분에서 실행된다.\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    #image = tf.image.random_saturation(image, 0, 2)\n",
        "    return image, label   \n",
        "\n",
        "def get_training_dataset():\n",
        "    dataset = load_dataset(TRAINING_FILENAMES)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.repeat()  # 훈련 데이터셋은 여러 에포크 동안 반복되어야 함\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO)  # 훈련하는 동안 다음 배치를 미리 준비(프리페치 버퍼 크기 자동 조정)\n",
        "    return dataset\n",
        "\n",
        "def get_validation_dataset(ordered=False):\n",
        "    dataset = load_dataset(VALIDATION_FILENAMES, ordered=ordered)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.prefetch(AUTO)  # 훈련하는 동안 다음 배치를 미리 준비(프리페치 버퍼 크기 자동 조정)\n",
        "    return dataset\n",
        "\n",
        "def count_data_items(filenames):\n",
        "    # 데이터 항목의 수는 .tfrec 파일의 이름으로 기록된다. 즉, flowers00-230.tfrec = 230 데이터 항목이다.\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\n",
        "NUM_VALIDATION_IMAGES = count_data_items(VALIDATION_FILENAMES)\n",
        "TRAIN_STEPS = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "STEPS_PER_EPOCH = NUM_TRAINING_IMAGES // BATCH_SIZE\n",
        "VALIDATION_STEPS = -(-NUM_VALIDATION_IMAGES // BATCH_SIZE) # The \"-(-//)\" trick rounds up instead of down :-)\n",
        "print('Dataset: {} training images, {} validation images'.format(NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8WcT2Kae0ye"
      },
      "source": [
        "# 데이터셋 시각화 (5 flowers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndOisDRpe0yf"
      },
      "outputs": [],
      "source": [
        "# 데이터 덤프\n",
        "print(\"Training data shapes:\")\n",
        "for image, label in get_training_dataset().take(3):\n",
        "    print(image.numpy().shape, label.numpy().shape)\n",
        "print(\"Training data label examples:\", label.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN7XDkEAe0yf"
      },
      "outputs": [],
      "source": [
        "# 훈련 데이터 엿보기\n",
        "training_dataset = get_training_dataset()\n",
        "training_dataset = training_dataset.unbatch().batch(20)\n",
        "train_batch = iter(training_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHm_x05fe0yf"
      },
      "outputs": [],
      "source": [
        "# 다음 이미지 세트를 위해 이 셀을 다시 실행\n",
        "display_batch_of_images(next(train_batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCRLijwue0yf"
      },
      "source": [
        "# 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKkqZdI8e0yf"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    pretrained_model = tf.keras.applications.MobileNetV2(\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=[*IMAGE_SIZE, 3])\n",
        "    \n",
        "    pretrained_model.trainable = True  # 파인 튜닝\n",
        "    \n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Lambda(  # 이미지 형식을 int[0,255]에서 이 모델이 예상하는 형식으로 변환\n",
        "            lambda data: tf.keras.applications.mobilenet.preprocess_input(\n",
        "                tf.cast(data, tf.float32)), input_shape=[*IMAGE_SIZE, 3]),\n",
        "        pretrained_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(16, activation='relu', name='flower_dense'),\n",
        "        tf.keras.layers.Dense(len(CLASSES), activation='softmax', name='flower_prob')\n",
        "    ])\n",
        "    \n",
        "    mult = 0.4  # 사전 훈련된 레이어의 경우\n",
        "    mult_by_layer={ \n",
        "            # 분류 헤드\n",
        "            'flower_prob': 1.0,\n",
        "            'flower_dense': 1.0,\n",
        "            # 사전 훈련된 레이어\n",
        "            'block_1_': 0.02 * mult,\n",
        "            'block_2_': 0.04 * mult,\n",
        "            'block_3_': 0.06 * mult,\n",
        "            'block_4_': 0.08 * mult,\n",
        "            'block_5_': 0.1 * mult,\n",
        "            'block_6_': 0.15 * mult,\n",
        "            'block_7_': 0.2 * mult,\n",
        "            'block_8_': 0.25 * mult,\n",
        "            'block_9_': 0.3 * mult,\n",
        "            'block_10_': 0.35 * mult,\n",
        "            'block_11_': 0.4 * mult,\n",
        "            'block_12_': 0.5 * mult,\n",
        "            'block_13_': 0.6 * mult,\n",
        "            'block_14_': 0.7 * mult,\n",
        "            'block_15_': 0.8 * mult,\n",
        "            'block_16_': 0.9 * mult,\n",
        "            # 이 레이어들은 tf.keras.applications.MobileNetV2에 안정적인 식별자가 없음\n",
        "            'conv': 0.5 * mult,\n",
        "            'Conv': 0.5 * mult\n",
        "    }\n",
        "        \n",
        "    optimizer = AdamW(lr=LR_MAX, model=model, lr_multipliers=mult_by_layer)\n",
        "    \n",
        "model.compile(\n",
        "    #optimizer='adam',\n",
        "    optimizer=optimizer,\n",
        "    loss = 'sparse_categorical_crossentropy',\n",
        "    metrics=['sparse_categorical_accuracy'],\n",
        "    steps_per_execution=8\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiTy1KX_e0yg"
      },
      "source": [
        "# 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kiwiSIQe0yg"
      },
      "outputs": [],
      "source": [
        "history = model.fit(get_training_dataset(), steps_per_epoch=STEPS_PER_EPOCH, epochs=EPOCHS,\n",
        "                    validation_data=get_validation_dataset(), validation_steps=VALIDATION_STEPS,\n",
        "                    callbacks=[lr_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga9mBUwie0yg"
      },
      "outputs": [],
      "source": [
        "display_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 211, ylim=[0,1.7])\n",
        "display_training_curves(history.history['sparse_categorical_accuracy'], history.history['val_sparse_categorical_accuracy'], 'accuracy', 212)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aog2J5DOe0yg"
      },
      "source": [
        "# 혼동 행렬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS0fAu0we0yg"
      },
      "outputs": [],
      "source": [
        "cmdataset = get_validation_dataset(ordered=True)  # 데이터셋을 분할하고 이미지와 레이블을 개별적으로 반복하므로 순서가 중요\n",
        "images_ds = cmdataset.map(lambda image, label: image)\n",
        "labels_ds = cmdataset.map(lambda image, label: label).unbatch()\n",
        "cm_correct_labels = next(iter(labels_ds.batch(NUM_VALIDATION_IMAGES))).numpy()  # 모든 것을 하나의 배치로 가져오기\n",
        "cm_probabilities = model.predict(images_ds, steps=VALIDATION_STEPS)\n",
        "cm_predictions = np.argmax(cm_probabilities, axis=-1)\n",
        "print(\"Correct   labels: \", cm_correct_labels.shape, cm_correct_labels)\n",
        "print(\"Predicted labels: \", cm_predictions.shape, cm_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPnXLBi3e0yg"
      },
      "outputs": [],
      "source": [
        "cmat = confusion_matrix(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)))\n",
        "score = f1_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
        "precision = precision_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
        "recall = recall_score(cm_correct_labels, cm_predictions, labels=range(len(CLASSES)), average='macro')\n",
        "cmat = (cmat.T / cmat.sum(axis=1)).T # normalized\n",
        "display_confusion_matrix(cmat, score, precision, recall)\n",
        "print('f1 score: {:.3f}, precision: {:.3f}, recall: {:.3f}'.format(score, precision, recall))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBzykiSpe0yg"
      },
      "source": [
        "# 시각적 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2licXMEe0yg"
      },
      "outputs": [],
      "source": [
        "dataset = get_validation_dataset()\n",
        "dataset = dataset.unbatch().batch(20)\n",
        "batch = iter(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k9u9bpve0yh"
      },
      "outputs": [],
      "source": [
        "# 다음 이미지 세트를 위해 이 셀을 다시 실행\n",
        "images, labels = next(batch)\n",
        "probabilities = model.predict(images)\n",
        "predictions = np.argmax(probabilities, axis=-1)\n",
        "display_batch_of_images((images, labels), predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjawOSXde0yh"
      },
      "source": [
        "## 라이선스\n",
        "Copyright 2021 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    }
  ],
  "metadata": {
    "environment": {
      "kernel": "python3",
      "name": "tf2-gpu.2-6.m87",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}